{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpsmLRD8uGrM8HeRS7wX/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushil-K/Deep-Learning/blob/main/ANN/nmrk2627_ANN_DLM_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Project 1 : Artificial Neural Networks**\n",
        "\n",
        "Contributors:\n",
        "- Rushil Kohli\n",
        "- Navneet Mittal\n"
      ],
      "metadata": {
        "id": "vGp05H9gwUTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Executive Summary: ANN Model for Conversion Rate Prediction**\n",
        "\n",
        "## **Project Overview**\n",
        "\n",
        "The goal of this project is to predict the conversion rate of customers based on a dataset of 1 million records. The primary objective is to build a predictive model using Artificial Neural Networks (ANN) to determine whether a customer will convert (i.e., make a purchase or take a desired action) based on various customer features. The model was then deployed using **Streamlit**, allowing for real-time predictions and insights through an interactive dashboard.\n",
        "\n",
        "This report provides a comprehensive overview of the entire process, from data curation to preprocessing, model training, and deployment.\n",
        "\n",
        "---\n",
        "\n",
        "## **Dataset Description**\n",
        "\n",
        "The dataset contains 1 million entries and 8 columns, with the following features:\n",
        "\n",
        "1. **CustomerID**: A unique identifier for each customer.\n",
        "\n",
        "2. **Age**: The age of the customer.\n",
        "\n",
        "3. **Gender**: The gender of the customer (categorical, encoded as 0 for Male and 1 for Female).\n",
        "\n",
        "4. **Income**: The income of the customer.\n",
        "\n",
        "5. **Purchases**: The number of purchases made by the customer.\n",
        "\n",
        "6. **Clicks**: The number of clicks made by the customer on advertisements or products.\n",
        "\n",
        "7. **Spent**: The amount of money spent by the customer.\n",
        "\n",
        "8. **Converted**: The binary target variable indicating if the customer converted (1) or not (0).\n",
        "\n",
        "The target variable, **Converted**, represents the conversion rate, and the other features represent various customer demographics and interactions. The dataset exhibits some class imbalance, with a higher proportion of non-converted customers.\n",
        "\n",
        "---\n",
        "\n",
        "## **Steps Taken**\n",
        "\n",
        "### **1\\. Data Curation**\n",
        "\n",
        "The dataset, containing 1 million records, was curated to ensure data quality and completeness. This involved ensuring that there were no missing values and that the data was clean for model training. The dataset contained both categorical (e.g., **Gender**) and continuous (e.g., **Income**, **Purchases**) variables.\n",
        "\n",
        "Given the large size of the dataset, careful consideration was given to how the data could be processed efficiently for model training. Additionally, class imbalance in the **Converted** target variable was acknowledged and addressed during preprocessing.\n",
        "\n",
        "### **2\\. Data Preprocessing**\n",
        "\n",
        "Data preprocessing is a critical step in preparing the dataset for machine learning, particularly when dealing with large datasets. The following preprocessing steps were performed:\n",
        "\n",
        "* **Encoding Categorical Features**:\n",
        "\n",
        "  * The **Gender** feature, which is categorical, was encoded into numeric values using **OrdinalEncoder**. Gender was represented as 0 for Male and 1 for Female.\n",
        "\n",
        "* **Handling Class Imbalance**:\n",
        "\n",
        "  * Since the target variable **Converted** was imbalanced (more non-converted customers than converted), **SMOTE (Synthetic Minority Over-sampling Technique)** was applied to oversample the minority class and balance the dataset.\n",
        "\n",
        "* **Standardization**:\n",
        "\n",
        "  * To ensure that all continuous variables had similar ranges and to improve the efficiency of training, **StandardScaler** was used to standardize features such as **Age**, **Income**, **Purchases**, **Clicks**, and **Spent**.\n",
        "\n",
        "* **Train-Test Split**:\n",
        "\n",
        "  * The data was split into a training set (80%) and a test set (20%) using **train\\_test\\_split** to ensure that the model could be tested on unseen data.\n",
        "\n",
        "* **Class Weights**:\n",
        "\n",
        "  * Class weights were computed using **compute\\_class\\_weight** to account for the class imbalance and ensure the model did not favor the majority class (non-converted customers).\n",
        "\n",
        "---\n",
        "\n",
        "### **3\\. Model Training**\n",
        "\n",
        "Once the data was preprocessed, the model training began. The following steps were taken during the training process:\n",
        "\n",
        "* **Model Architecture**:\n",
        "\n",
        "  * A **Sequential** ANN model was built using **TensorFlow** and **Keras**. The architecture consisted of an input layer followed by several dense layers with **ReLU** activation functions, and dropout layers to prevent overfitting.\n",
        "\n",
        "  * The final output layer was a single neuron with a **sigmoid** activation function to classify the data into two classes: converted (1) and not converted (0).\n",
        "\n",
        "* **Hyperparameter Selection**:\n",
        "\n",
        "  * Various hyperparameters, such as **epochs**, **learning rate**, **optimizer**, **activation function**, **dropout rate**, and **number of neurons per layer**, were selected through the Streamlit UI.\n",
        "\n",
        "  * The model used optimizers like **Adam**, **SGD**, and **RMSProp**, with the learning rate being set from options: `0.01`, `0.001`, or `0.0001`.\n",
        "\n",
        "  * A range of values for **dropout rates** (0.1 to 0.5) and **dense layers** (2 to 5 layers) were provided for experimentation.\n",
        "\n",
        "* **Training**:\n",
        "\n",
        "  * The model was trained using the preprocessed training data, with a **batch size of 128** and **epochs** selected based on user input through the Streamlit interface. The model also incorporated **class weights** to address class imbalance.\n",
        "\n",
        "  * **Early stopping** was not used, but the model was trained for a specified number of epochs with **validation split** set to 0.2 to monitor performance during training.\n",
        "\n",
        "---\n",
        "\n",
        "### **4\\. Model Evaluation**\n",
        "\n",
        "After training, the model was evaluated using the following metrics:\n",
        "\n",
        "* **Test Accuracy**: The accuracy of the model on the test dataset was calculated to evaluate how well the model generalized to unseen data.\n",
        "\n",
        "* **Test Loss**: The binary cross-entropy loss on the test dataset was measured to assess the model's error rate.\n",
        "\n",
        "In addition to these metrics, the following evaluation tools were used:\n",
        "\n",
        "* **Training Performance Plots**:\n",
        "\n",
        "  * **Accuracy** and **Loss** curves were plotted over epochs to visualize the model’s learning performance. This allowed the identification of any overfitting or underfitting trends.\n",
        "\n",
        "* **Confusion Matrix**:\n",
        "\n",
        "  * A confusion matrix was created to visualize how well the model predicted each class (converted or not converted). This provided insight into false positives and false negatives.\n",
        "\n",
        "* **Classification Report**:\n",
        "\n",
        "  * A detailed classification report was generated, showing **precision**, **recall**, and **F1-score** for both the converted and non-converted classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **5\\. Model Interpretation**\n",
        "\n",
        "To interpret the model's predictions and understand feature importance:\n",
        "\n",
        "* **SHAP (Shapley Additive Explanations)**:\n",
        "\n",
        "  * **SHAP values** were calculated for a subset of the test data to explain the model’s predictions. The **summary plot** displayed which features had the most significant impact on conversion predictions.\n",
        "\n",
        "  * The **mean absolute SHAP values** were calculated to rank features by their importance.\n",
        "\n",
        "---\n",
        "\n",
        "### **6\\. Dashboard Creation**\n",
        "\n",
        "After training and evaluation, the model was deployed through a **Streamlit** dashboard. The dashboard provides an interactive interface for users to:\n",
        "\n",
        "* **Train the Model**:\n",
        "\n",
        "  * The user can adjust hyperparameters (e.g., learning rate, number of layers, dropout rate) and retrain the model on the data.\n",
        "\n",
        "* **Visualize Model Performance**:\n",
        "\n",
        "  * The dashboard displays various performance metrics like **test accuracy**, **test loss**, and **confusion matrix**.\n",
        "\n",
        "  * **Training performance plots** for accuracy and loss over epochs are shown to visualize the model’s learning.\n",
        "\n",
        "* **View Evaluation Metrics**:\n",
        "\n",
        "  * The **classification report** and **feature importance** are available for deeper insights into the model’s behavior.\n",
        "\n",
        "* **Feature Importance**:\n",
        "\n",
        "  * The **SHAP values** are displayed, indicating the most influential features in the conversion prediction.\n",
        "\n",
        "The Streamlit dashboard enables users to easily interact with the trained model and explore the results visually.\n",
        "\n",
        "---\n",
        "\n",
        "## **Tech Stack**\n",
        "\n",
        "* **Google Colab**: Used for data preprocessing, model training, and experimentation.\n",
        "\n",
        "* **TensorFlow** and **Keras**: Used to build and train the Artificial Neural Network (ANN).\n",
        "\n",
        "* **Streamlit**: Deployed the model and created an interactive dashboard for visualization and prediction.\n",
        "\n",
        "* **Scikit-learn**: Utilized for data preprocessing, model evaluation, and performance metrics such as confusion matrix and classification report.\n",
        "\n",
        "* **SHAP**: Used to interpret the model’s predictions and identify feature importance.\n",
        "\n",
        "* **SMOTE**: Applied to address the class imbalance by oversampling the minority class.\n",
        "\n",
        "* **gdown**: Used to download the dataset from Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "This project demonstrates the ability of an Artificial Neural Network (ANN) to predict customer conversion rates based on various customer features. Through the preprocessing steps, class imbalance was addressed, and the model was trained with a carefully selected set of hyperparameters. The model achieved an accuracy of around **50-65%**, providing a reliable starting point for predicting customer conversion.\n",
        "\n",
        "The interactive **Streamlit dashboard** allows for easy exploration of the model’s performance, evaluation metrics, and feature importance. This project provides valuable insights into customer behavior, and further optimization could improve model performance, making it a practical tool for marketing teams looking to predict customer conversion rates."
      ],
      "metadata": {
        "id": "miiaggVlddzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "l0d7KQZoyACX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "import io\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "3n-0_RT74ITH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual file ID\n",
        "file_id = '1OPmMFUQmeZuaiYb0FQhwOMZfEbVrWKEK'\n",
        "\n",
        "# Construct the URL for direct download (using export)\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Fetch the data using requests\n",
        "\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "# Read the data into a pandas DataFrame using StringIO\n",
        "# Specify encoding if needed, e.g., encoding='latin1' or encoding='utf-8'\n",
        "nmrk2627_df = pd.read_csv(StringIO(response.text), encoding='utf-8')\n",
        "\n",
        "# Display the head of the dataframe to verify data loading.\n",
        "display(nmrk2627_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ibzuZHpRyj1_",
        "outputId": "28b3eb92-76ba-4c34-bd40-21f0b37c5d33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   CustomerID  Age  Gender   Income  Purchases  Clicks   Spent  Converted\n",
              "0           1   41  Female  52618.0         26      67  2434.0          0\n",
              "1           2   43    Male  53114.0          3      14  2937.0          0\n",
              "2           3   43  Female  96145.0          4      78  2076.0          0\n",
              "3           4   35  Female  92590.0         10      13  1437.0          1\n",
              "4           5   23  Female  69262.0         14      62  1675.0          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1280d98a-43a5-4f90-84da-b0b6633dd9e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Income</th>\n",
              "      <th>Purchases</th>\n",
              "      <th>Clicks</th>\n",
              "      <th>Spent</th>\n",
              "      <th>Converted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>Female</td>\n",
              "      <td>52618.0</td>\n",
              "      <td>26</td>\n",
              "      <td>67</td>\n",
              "      <td>2434.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>Male</td>\n",
              "      <td>53114.0</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>2937.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>43</td>\n",
              "      <td>Female</td>\n",
              "      <td>96145.0</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>Female</td>\n",
              "      <td>92590.0</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>1437.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>Female</td>\n",
              "      <td>69262.0</td>\n",
              "      <td>14</td>\n",
              "      <td>62</td>\n",
              "      <td>1675.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1280d98a-43a5-4f90-84da-b0b6633dd9e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1280d98a-43a5-4f90-84da-b0b6633dd9e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1280d98a-43a5-4f90-84da-b0b6633dd9e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-caa64255-fb95-47cd-9410-a3679f67dbdf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-caa64255-fb95-47cd-9410-a3679f67dbdf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-caa64255-fb95-47cd-9410-a3679f67dbdf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(nmrk2627_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"CustomerID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 23,\n        \"max\": 43,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          43,\n          23,\n          41\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20880.538790941195,\n        \"min\": 52618.0,\n        \"max\": 96145.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          53114.0,\n          69262.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Purchases\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 3,\n        \"max\": 26,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Clicks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 13,\n        \"max\": 78,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          14,\n          62\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spent\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 598.3725428192707,\n        \"min\": 1437.0,\n        \"max\": 2937.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2937.0,\n          1675.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Converted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmrk2627_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw6SA9FI64XB",
        "outputId": "2d9dc0ee-c8d5-41e6-d96a-ccff7234f0c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 8 columns):\n",
            " #   Column      Non-Null Count    Dtype  \n",
            "---  ------      --------------    -----  \n",
            " 0   CustomerID  1000000 non-null  int64  \n",
            " 1   Age         1000000 non-null  int64  \n",
            " 2   Gender      1000000 non-null  object \n",
            " 3   Income      1000000 non-null  float64\n",
            " 4   Purchases   1000000 non-null  int64  \n",
            " 5   Clicks      1000000 non-null  int64  \n",
            " 6   Spent       1000000 non-null  float64\n",
            " 7   Converted   1000000 non-null  int64  \n",
            "dtypes: float64(2), int64(5), object(1)\n",
            "memory usage: 61.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANALYSIS**"
      ],
      "metadata": {
        "id": "Y2nOuqYoLmdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam Optimizer"
      ],
      "metadata": {
        "id": "EoBzKtiwWk6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Data Preprocessing and Feature Engineering:\n",
        "\n",
        "# Assuming 'nmrk2627_df' is your DataFrame\n",
        "X = nmrk2627_df.drop('Converted', axis=1)\n",
        "y = nmrk2627_df['Converted']\n",
        "\n",
        "# One-hot encode 'Gender'\n",
        "X = pd.get_dummies(X, columns=['Gender'], drop_first=True)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=552627)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=552627)\n",
        "\n",
        "# Standardize numerical features for training set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Standardize validation and test sets using training set's statistics\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Handle class imbalance using SMOTE only on training set\n",
        "smote = SMOTE(random_state=552627)  # Using consistent random state\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Add polynomial features (degree=2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train = poly.fit_transform(X_train)\n",
        "X_val = poly.transform(X_val)\n",
        "X_test = poly.transform(X_test)\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# 2. Model Building and Training:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(BatchNormalization())  # Added Batch Normalization\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())  # Added Batch Normalization\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Experiment with optimizers and learning rates\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "# optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Use Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # Increased patience\n",
        "\n",
        "# Save the best model during training\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train the model with a reasonable number of epochs and class weights\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks=[early_stopping, model_checkpoint],\n",
        "          class_weight=class_weights_dict)  # Add class_weight\n",
        "\n",
        "\n",
        "# 3. Prediction and Evaluation:\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Generate confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBONiVJdUgty",
        "outputId": "ea6fe210-2b1e-4dc6-b4ee-d030911e374c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5011 - loss: 0.8541"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.5011 - loss: 0.8540 - val_accuracy: 0.3642 - val_loss: 0.7098\n",
            "Epoch 2/10\n",
            "\u001b[1m3275/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5040 - loss: 0.6996"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.5040 - loss: 0.6996 - val_accuracy: 0.3770 - val_loss: 0.6977\n",
            "Epoch 3/10\n",
            "\u001b[1m3273/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5030 - loss: 0.6933"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5030 - loss: 0.6933 - val_accuracy: 0.3901 - val_loss: 0.6977\n",
            "Epoch 4/10\n",
            "\u001b[1m3270/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5045 - loss: 0.6931"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5045 - loss: 0.6931 - val_accuracy: 0.4037 - val_loss: 0.6967\n",
            "Epoch 5/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5059 - loss: 0.6931 - val_accuracy: 0.3923 - val_loss: 0.6968\n",
            "Epoch 6/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5066 - loss: 0.6930 - val_accuracy: 0.3827 - val_loss: 0.6971\n",
            "Epoch 7/10\n",
            "\u001b[1m3263/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5057 - loss: 0.6931"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.5057 - loss: 0.6931 - val_accuracy: 0.4060 - val_loss: 0.6962\n",
            "Epoch 8/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5073 - loss: 0.6929 - val_accuracy: 0.3800 - val_loss: 0.6981\n",
            "Epoch 9/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.5085 - loss: 0.6929 - val_accuracy: 0.3858 - val_loss: 0.6977\n",
            "Epoch 10/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5086 - loss: 0.6929 - val_accuracy: 0.3831 - val_loss: 0.6987\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n",
            "Accuracy: 0.40476\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.26      0.38    139826\n",
            "           1       0.30      0.73      0.43     60174\n",
            "\n",
            "    accuracy                           0.40    200000\n",
            "   macro avg       0.50      0.50      0.40    200000\n",
            "weighted avg       0.58      0.40      0.40    200000\n",
            "\n",
            "[[ 36784 103042]\n",
            " [ 16006  44168]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uBq7hTwwXJA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Data Preprocessing and Feature Engineering:\n",
        "\n",
        "# Assuming 'nmrk2627_df' is your DataFrame\n",
        "X = nmrk2627_df.drop('Converted', axis=1)\n",
        "y = nmrk2627_df['Converted']\n",
        "\n",
        "# One-hot encode 'Gender'\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Create OneHotEncoder\n",
        "encoded_gender = encoder.fit_transform(X[['Gender']])  # Fit and transform Gender column\n",
        "gender_df = pd.DataFrame(encoded_gender, columns=encoder.get_feature_names_out(['Gender']))  # Create DataFrame\n",
        "X = X.drop('Gender', axis=1)  # Drop original Gender column\n",
        "X = pd.concat([X, gender_df], axis=1)  # Concatenate encoded Gender\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=552627)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=552627)\n",
        "\n",
        "# Standardize numerical features for training set\n",
        "numerical_features = ['Age', 'Income', 'Purchases', 'Clicks', 'Spent']\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
        "\n",
        "# Standardize validation and test sets using training set's statistics\n",
        "X_val[numerical_features] = scaler.transform(X_val[numerical_features])\n",
        "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Handle class imbalance using SMOTE only on training set\n",
        "smote = SMOTE(random_state=552627)  # Using consistent random state\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# 2. Model Building and Training:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Use Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Save the best model during training\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train the model and store the history\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=256,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping, model_checkpoint],\n",
        "                    class_weight=class_weights_dict)  # Add class_weight\n",
        "\n",
        "\n",
        "# 3. Prediction and Evaluation:\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Generate confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY8HO9FcXIrZ",
        "outputId": "cb8c479c-e2b2-4766-b03b-2d461a7b1da0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4996 - loss: 961.8517"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.4996 - loss: 961.6132 - val_accuracy: 0.7001 - val_loss: 0.6915\n",
            "Epoch 2/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - accuracy: 0.4986 - loss: 0.8123 - val_accuracy: 0.2999 - val_loss: 0.6956\n",
            "Epoch 3/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.7450 - val_accuracy: 0.7001 - val_loss: 0.6926\n",
            "Epoch 4/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.4999 - loss: 0.7053 - val_accuracy: 0.2999 - val_loss: 0.6934\n",
            "Epoch 5/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.6989 - val_accuracy: 0.7001 - val_loss: 0.6923\n",
            "Epoch 6/10\n",
            "\u001b[1m3276/3276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.4987 - loss: 0.7008 - val_accuracy: 0.7001 - val_loss: 0.6924\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n",
            "Accuracy: 0.69913\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82    139826\n",
            "           1       0.00      0.00      0.00     60174\n",
            "\n",
            "    accuracy                           0.70    200000\n",
            "   macro avg       0.35      0.50      0.41    200000\n",
            "weighted avg       0.49      0.70      0.58    200000\n",
            "\n",
            "[[139826      0]\n",
            " [ 60174      0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Data Preprocessing and Feature Engineering:\n",
        "\n",
        "# Assuming 'nmrk2627_df' is your DataFrame\n",
        "X = nmrk2627_df.drop('Converted', axis=1)\n",
        "y = nmrk2627_df['Converted']\n",
        "\n",
        "# One-hot encode 'Gender'\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Create OneHotEncoder\n",
        "encoded_gender = encoder.fit_transform(X[['Gender']])  # Fit and transform Gender column\n",
        "gender_df = pd.DataFrame(encoded_gender, columns=encoder.get_feature_names_out(['Gender']))  # Create DataFrame\n",
        "X = X.drop('Gender', axis=1)  # Drop original Gender column\n",
        "X = pd.concat([X, gender_df], axis=1)  # Concatenate encoded Gender\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=552627)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=552627)\n",
        "\n",
        "# Standardize numerical features for training set\n",
        "numerical_features = ['Age', 'Income', 'Purchases', 'Clicks', 'Spent']\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
        "\n",
        "# Standardize validation and test sets using training set's statistics\n",
        "X_val[numerical_features] = scaler.transform(X_val[numerical_features])\n",
        "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Replace SMOTE with ADASYN\n",
        "adasyn = ADASYN(random_state=552627)\n",
        "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# 2. Model Building and Training:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Use Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Save the best model during training\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train the model and store the history\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=256,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stopping, model_checkpoint],\n",
        "                    class_weight=class_weights_dict)  # Add class_weight\n",
        "\n",
        "\n",
        "# 3. Prediction and Evaluation:\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Generate confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ftz_1G-cV3P",
        "outputId": "6d90bf7d-ac5a-4564-fdbd-f1e7a85ed7db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4992 - loss: 1340.9580"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.4992 - loss: 1340.6409 - val_accuracy: 0.7001 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.4975 - loss: 0.9185 - val_accuracy: 0.2999 - val_loss: 0.6955\n",
            "Epoch 3/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5071 - loss: 0.7390 - val_accuracy: 0.7001 - val_loss: 0.6929\n",
            "Epoch 4/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.4990 - loss: 0.7176 - val_accuracy: 0.2999 - val_loss: 0.6948\n",
            "Epoch 5/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5072 - loss: 0.6948 - val_accuracy: 0.7001 - val_loss: 0.6926\n",
            "Epoch 6/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.5024 - loss: 0.7301 - val_accuracy: 0.7001 - val_loss: 0.6923\n",
            "Epoch 7/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.5000 - loss: 0.7060 - val_accuracy: 0.2999 - val_loss: 0.6961\n",
            "Epoch 8/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.4985 - loss: 0.6954 - val_accuracy: 0.7001 - val_loss: 0.6922\n",
            "Epoch 9/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5058 - loss: 0.6981 - val_accuracy: 0.7001 - val_loss: 0.6923\n",
            "Epoch 10/10\n",
            "\u001b[1m3397/3397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.4992 - loss: 0.6934 - val_accuracy: 0.7001 - val_loss: 0.6917\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
            "Accuracy: 0.69913\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82    139826\n",
            "           1       0.00      0.00      0.00     60174\n",
            "\n",
            "    accuracy                           0.70    200000\n",
            "   macro avg       0.35      0.50      0.41    200000\n",
            "weighted avg       0.49      0.70      0.58    200000\n",
            "\n",
            "[[139826      0]\n",
            " [ 60174      0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}